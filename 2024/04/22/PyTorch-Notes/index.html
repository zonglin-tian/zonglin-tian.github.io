

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid_theme/fluid.png">
  <link rel="icon" href="/img/fluid_theme/conan_b.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Temm">
  <meta name="keywords" content="奋斗 自律 完美主义">
  
    <meta name="description" content="Pytorch study notes">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch Notes">
<meta property="og:url" content="https://zonglin-tian.github.io/2024/04/22/PyTorch-Notes/index.html">
<meta property="og:site_name" content="Temm&#39;s Zone">
<meta property="og:description" content="Pytorch study notes">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zonglin-tian.github.io/img/index/pytorch.png">
<meta property="article:published_time" content="2024-04-22T14:11:34.000Z">
<meta property="article:modified_time" content="2024-09-04T15:32:28.559Z">
<meta property="article:author" content="Temm">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://zonglin-tian.github.io/img/index/pytorch.png">
  
  
  
  <title>PyTorch Notes - Temm&#39;s Zone</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/research-icon/iconfont.css">
<link rel="stylesheet" href="/css/writing/writing_style.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"zonglin-tian.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":50,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":2},"lazyload":{"enable":true,"loading_img":"/img/fluid_theme/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"TTUkpi4yPOFPvGdrwRvjUPy5-gzGzoHsz","app_key":"ysSAYFHFwtsVmtCrHzIPHVJg","server_url":"https://ttukpi4y.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  <script src="/live2d/autoload.js"></script><link rel="stylesheet" href="/css/custom-tag/timeline.css"><meta name="google-site-verification" content="e8wFrBN9eGE2sBgp38s16D-NZxnwfcFMtpKqItkj5T4"/>
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Half_Perfect</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-fenlei1"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/grocery/" target="_self">
                <i class="iconfont icon-wodesuibi"></i>
                <span>随笔</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/research/" target="_self">
                <i class="iconfont icon-boshimao-F"></i>
                <span>科研</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/fluid_theme/lm.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.5)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="PyTorch Notes"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-22 22:11" pubdate>
          2024年4月22日 晚上
        </time>
      </span>
    
    <!-- added by Temm -->
    
    <!-- added by Temm -->
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          1.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          10 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="Coding"
        id="heading-cddcade0405954c4325ee1fb11e3dc0e" role="tab" data-toggle="collapse" href="#collapse-cddcade0405954c4325ee1fb11e3dc0e"
        aria-expanded="true"
      >
        Coding
        <span class="list-group-count">(6)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-cddcade0405954c4325ee1fb11e3dc0e"
           role="tabpanel" aria-labelledby="heading-cddcade0405954c4325ee1fb11e3dc0e">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/2024/07/31/Numpy-notes/" title="NumPy notes"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">NumPy notes</span>
        </a>
      
    
      
      
        <a href="/2024/04/22/PyTorch-Notes/" title="PyTorch Notes"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">PyTorch Notes</span>
        </a>
      
    
      
      
        <a href="/2024/05/05/Python-%E5%9F%BA%E7%A1%80/" title="Python 基础"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Python 基础</span>
        </a>
      
    
      
      
        <a href="/2024/05/05/Python-%E5%BA%93%E5%AD%A6%E4%B9%A0/" title="Python 库学习"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Python 库学习</span>
        </a>
      
    
      
      
        <a href="/2024/09/01/Tensorboard-notes/" title="Tensorboard notes"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">Tensorboard notes</span>
        </a>
      
    
      
      
        <a href="/2024/10/25/%E7%BB%88%E7%AB%AF-Python-IPython/" title="终端 Python: IPython"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">终端 Python: IPython</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">PyTorch Notes</h1>
            
              <p id="updated-time" class="note note-info" style="display: none">
                
                  
                    最后更新于: 2024-09-04T23:32:28+08:00
                  
                  

                
              </p>
            
            
              <div class="markdown-body">
                
                <h1 id="安装与验证"><a href="#安装与验证" class="headerlink" title="安装与验证"></a>安装与验证</h1><p>通过<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">官网</a>指南安装, 如 CUDA 11.3 + PyTorch 1.12</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 视觉任务通常不需要安装 torchaudio 包</span><br>conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch<br></code></pre></td></tr></table></figure>
<p><strong>验证:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 验证 torch 是否正常安装</span><br>ipython  <span class="hljs-comment"># pip install ipython, ipython 比 python 输入更方便</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-built_in">print</span>(torch.rand(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>))<br><br><span class="hljs-comment"># 验证是否支持 GPU 加速</span><br><span class="hljs-built_in">print</span>(torch.cuda.is_available())<br></code></pre></td></tr></table></figure>

<h1 id="运行配置"><a href="#运行配置" class="headerlink" title="运行配置"></a>运行配置</h1><ul>
<li><code>CUDA_VISIBLE_DEVICES</code>: 环境变量, 控制哪些 GPU 设备可用于 CUDA 程序 (可在程序运行时指定, 或者在程序中使用 <code>os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;])</code> 定义)</li>
</ul>
<h1 id="The-Basics"><a href="#The-Basics" class="headerlink" title="The Basics"></a>The Basics</h1><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p><code>Tensors</code> 是一种特殊的数据结构, 与数组和矩阵非常相似, 在 PyTorch 中, <em>使用 Tensors 表示模型的输入和输出以及模型的参数</em></p>
<ul>
<li>Tensors 与 NumPy 中的 <code>ndarrays</code> 类似, 只是 Tensors 可以在 GPU 或其它硬件加速器上运行</li>
<li>Tensors 可以和 ndarray 共享相同的底层内存, 从而无需复制数据</li>
<li>Tensors 对自动微分进行了优化</li>
</ul>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li><em><strong>shape: 整数元组</strong></em>  –&gt; <code>Tensor.size()</code></li>
<li>dtype</li>
<li>device: 存储设备 (默认 cpu)</li>
<li>dimensions: <code>Tensor.dim()=len(Tensor.shape)</code></li>
<li>total numbel of elments: <code>Tensor.numel()=torch.numel(tensor)</code></li>
</ul>
<h3 id="常见初始化操作"><a href="#常见初始化操作" class="headerlink" title="常见初始化操作"></a>常见初始化操作</h3><ul>
<li>用 <code>tensor</code> 函数转化 python 序列<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]]<br>t_data = torch.tensor(data)<br></code></pre></td></tr></table></figure></li>
<li>用 <code>from_numpy/tensor</code> 函数转化 NumPy 数组<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>t_np = torch.from_numpy(np_array)<br><span class="hljs-comment"># 不共享内存</span><br>t_np = torch.tensor(np_array)<br><span class="hljs-comment"># 将 tensor 转化为 NumPy 数组</span><br>np_t = tensor_var.numpy()<br></code></pre></td></tr></table></figure></li>
<li>已知 <code>shape</code>, 利用特殊函数创建<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>t_ones = torch.ones(shape)<br>t_zeros = torch.zeros(shape)<br>t_rand = torch.rand(shape, dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># 指定数据类型</span><br></code></pre></td></tr></table></figure></li>
<li>参考已知 Tensors, 利用 <code>*_like</code> 函数创建<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">t_ones = torch.ones_like(t_data)<br>t_zeros = torch.zeros_like(t_data)<br>t_rand = torch.rand_like(t_data, dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># 指定数据类型</span><br></code></pre></td></tr></table></figure></li>
</ul>
<h3 id="Tensor-创建函数"><a href="#Tensor-创建函数" class="headerlink" title="Tensor 创建函数"></a>Tensor 创建函数</h3><ul>
<li>torch.empty(*size, dtype&#x3D;None): 创建未初始化的 Tensor</li>
<li>torch.zeros(*size, dtype&#x3D;None)</li>
<li>torch.zeros_like(input, dtype&#x3D;None)</li>
<li>torch.ones(*size, dtype&#x3D;None)</li>
<li>torch.randn(*size, dtype&#x3D;None): 随机数据满足标准正太分布</li>
<li>torch.rand(*size, dtype&#x3D;None): 随机数据满足 (0, 1) 上的均匀分布</li>
<li>torh.normal(means, std) 离散正态分布</li>
<li>torch.linspace(start, end, steps): create a one-dimensonal tensor of <code>steps</code> whose value evenly spaced from <code>start</code> to <code>end</code></li>
<li>torch.arange(start&#x3D;0, end, step&#x3D;1, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): return a 1-D tensor of size <code>(end - start) // step</code> with values from the <code>[start, end)</code> taken with common difference <code>step</code> beginnig from <code>start</code></li>
<li>torch.tensor(data, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): 用数据构造一个张量</li>
<li>torch.Tensor(data): 用数据构造一个张量</li>
</ul>
<p><strong>Note:</strong></p>
<div class="note note-warning">
            <p>torch.tensor() 是一个函数, 使用时对输入进行拷贝 (不是直接引用), 并根据<em>原始数据</em>生成相应的 torch.LongTensor, torch.FloatTensor, torch.DoubleTensor<br>torch.Tensor() 是一个类 (默认张量类型 torch.FloatTensor() 的别名), 使用时会调用 Tensor 类的构造函数 <strong>init</strong>, 生成<em>单精度浮点类型的张量, 也可仅指定 shape, 此时其可以看作 torch.empty() 的一个特例</em></p>
          </div>

<h3 id="Tensor-的数据类型"><a href="#Tensor-的数据类型" class="headerlink" title="Tensor 的数据类型"></a>Tensor 的数据类型</h3><ul>
<li>torh.FloatTensor() 或 torch.Tenosr(): 32 位浮点数</li>
<li>torch.DoubleTenosr(): 64 位浮点数</li>
<li>torch.ShortTensor(): 16 位整型</li>
<li>torch.IntTensor(): 32 位整型</li>
<li>torch.LongTensor(): 64 位整型</li>
</ul>
<p><strong>数据类型转化:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">tesor = torch.Tensor(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)  <span class="hljs-comment"># 随机生成 3*5 的 tensor</span><br><span class="hljs-comment"># 转成 long 类型</span><br>newtensor = tensor.long()<br><span class="hljs-comment"># 转成半精度浮点类型 (float16)</span><br>newtensor = tensor.half()<br><span class="hljs-comment"># 转成 int 类型</span><br>newtensor = tensor.<span class="hljs-built_in">int</span>()<br><span class="hljs-comment"># 转成 short 类型</span><br>newtensor = tensor.short()<br><span class="hljs-comment"># 转成 double 类型</span><br>newtensor = tensor.double()<br><span class="hljs-comment"># 转成 float 类型</span><br>newtensor = tensor.<span class="hljs-built_in">float</span>()<br><span class="hljs-comment"># 转成 char 类型</span><br>newtensor = tensor.char()<br><span class="hljs-comment"># 转成 byte 类型</span><br>newtensor = tensor.byte()<br></code></pre></td></tr></table></figure>

<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul>
<li>Tensor.tolist(): return the tensor as a (nested) list. For scalars, a standard Python number is returned like <code>Tensor.item()</code> (only work for tensors with one element)</li>
<li>Tensor.item(): return the value of this tensor as a standard Python number</li>
<li>Tensor.numpy(): 将 Tensor 类型转变为 numpy 类型</li>
<li>Tensor.to(*arg, **kwargs): change an existing tensor’s <code>torch.device</code> and <code>torch.dtype</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>tensor.to(torch.float64)<br>cuda0 = torch.device(<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br>tensor.to(cuda0)<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="操作中参数-dim-的理解"><a href="#操作中参数-dim-的理解" class="headerlink" title="操作中参数 dim 的理解"></a>操作中参数 <code>dim</code> 的理解</h2><p><strong>The way to understand the “axis” in NumPy or “dim” in PyTorch is that it <em>collapses the specified axis. So when it collapses</em> the axis 0 (the row), it becomes just one row (column-wise)</strong></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42782150/article/details/106862236">Yale曼陀罗. PyTorch学习笔记——Tensor张量的数据类型的转化、Tensor常见的数据类型、快速创建Tensor. CSDN</a><br>[2] <a target="_blank" rel="noopener" href="https://mathpretty.com/12065.html">王茂南. 理解 PyTorch 中维度的概念</a><br>[3] <a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be">Boyan Barakov. Understanding dimensions in PyTorch</a></p>
<h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><ul>
<li>torch.mean(input): return the mean value of <strong>all</strong> elements in the <code>input</code> tensor</li>
<li>torch.tensor(data, *, dtype&#x3D;None, device&#x3D;None, requires_grad&#x3D;False): construct a tensor with <em>no</em> autograd history</li>
<li>torch.as_tensor(data, dtype&#x3D;None, device&#x3D;None): convert data into a tensor, sharing data and preserving autograd histroy <em>if possible</em><ul>
<li>torch.from_numpy(ndarray): create a Tensor from a numpy.ndarray, sharing the same memory</li>
</ul>
</li>
<li>torch.log(input): return a new tensor with the natural logirthm of the <em>elements</em> of input, $y_i &#x3D; \ln x_i$<!-- STOP HERE: 2024-04-24/22:32 --></li>
<li>torch.stack(): 沿着一个新维度对输入张量序列进行连接, 序列中所有的张量具有相同形状</li>
<li>torh.flatten(input, start_dim&#x3D;0, end_dim&#x3D;-1): reshape the input into a one-dimensional tensor</li>
<li>torch.split(tensor, split_size_or_sections, dim&#x3D;0): split the tensor into chunks</li>
<li>torch.index_select(input, dim, index, *, out&#x3D;None): <ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_50001789/article/details/120315215">视觉萌新. torch.index_select()——数组索引. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.index_select.html">TORCH.INDEX_SELECT</a></li>
</ul>
</li>
<li>torch.cross(input, other, dim&#x3D;None): return the cross product of vectors in dimension <code>dim</code> of <code>input</code> and <code>other</code><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.cross.html">TORCH.CROSS</a></li>
</ul>
</li>
<li>torch.outer(input, vec2): outer product of input and vec2 (可通过坐标生成坐标网格)</li>
<li>torch.inverse(A) 或者 torch.linalg.inv(A): 返回矩阵 A 的逆</li>
<li>torch.matmul(A, B) 或者 A @ B: 矩阵相乘<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/beauthy/article/details/121103704">柏常青. pytorch中的矩阵乘法：函数mul,mm,mv以及 @运算 和 *运算. CSDN</a></li>
</ul>
</li>
</ul>
<h2 id="数据加载-utils-data"><a href="#数据加载-utils-data" class="headerlink" title="数据加载 utils.data"></a>数据加载 utils.data</h2><ul>
<li><code>DataLoader</code>: <code>__len__(), __get_item__()</code></li>
<li><code>Sampler</code>: <code>__init__(), __iter__()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取 train_data_loader 的某个 batch</span><br>batch = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_data_loader))<br>target_batch_index = <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> idx, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_data_loader):<br>    <span class="hljs-keyword">if</span> idx == target_batch_index:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Batch Index:&quot;</span>, idx)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Batch Data:&quot;</span>, batch)<br></code></pre></td></tr></table></figure>

<h2 id="copy-与-detach"><a href="#copy-与-detach" class="headerlink" title="copy 与 detach"></a>copy 与 detach</h2><p>When we wan to graph some of our tensors. We may have a tensor that requires gradient tracking, but you want a (shallow) copy that does not. This is because <code>matplotlib</code> expects a NumPy array as input, and the implict conversion from a PyTorch tensor to a NumPy array is <strong>not enabled for tensors with <code>requires_grad=True</code></strong>. Making a <code>detached copy</code> lets us move forward.</p>
<h2 id="模型加载与保存"><a href="#模型加载与保存" class="headerlink" title="模型加载与保存"></a>模型加载与保存</h2><ul>
<li>torch.save()</li>
<li>torch.load()<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40522801/article/details/106563354">宁静致远*. Pytorch：模型的保存与加载. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82038049">鑫鑫淼淼焱焱. PyTorch | 保存和加载模型. 知乎</a></li>
</ul>
</li>
</ul>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">TORCH.OPTIM</a></li>
</ul>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qyhaill/article/details/103043637?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-103043637-blog-109841612.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-103043637-blog-109841612.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=3">八块腹肌怎么练. torch.optim.lr_scheduler：调整学习率. CSDN</a></li>
</ul>
<h2 id="nn"><a href="#nn" class="headerlink" title="nn"></a>nn</h2><ul>
<li>CrossEntropyLoss<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36201400/article/details/111335423?spm=1001.2101.3001.6650.10&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-111335423-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-10-111335423-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=13">仁义礼智信达. torch.nn.CrossEntropyLoss()用法. CSDN</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45414792/article/details/120778065?spm=1001.2101.3001.6650.13&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-13-120778065-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-13-120778065-blog-124689632.235%5Ev38%5Epc_relevant_sort_base1&utm_relevant_index=16">有点聪明的亚子1. nn.CrossEntropyLoss()交叉熵损失函数. CSDN</a></li>
</ul>
</li>
</ul>
<h2 id="distributed"><a href="#distributed" class="headerlink" title="distributed"></a>distributed</h2><ul>
<li>基本概念<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76638962">Pytorch 分布式训练</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hxxjxw/article/details/119606518">DPP的基本概念</a></li>
</ul>
</li>
<li><code>launch</code>: 分布式训练运行命令<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/magic_ll/article/details/122359490">torch.distributed.launch 命令</a></li>
</ul>
</li>
<li><code>init_process_group()</code>: 分布式训练初始化<ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38252409/article/details/134965424">torch.distributed.init_process_group()详细说明</a></li>
</ul>
</li>
</ul>
<h1 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h1><h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><ul>
<li>make_grid(tensor, nrow&#x3D;8, padding&#x3D;2, normalize&#x3D;False, range&#x3D;None, sacle_each&#x3D;False, pad_value&#x3D;0): make a grid of images<ul>
<li>tensor [B, C, H, W]</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html">torchvision.utils.make_grid</a></li>
</ul>
</li>
<li></li>
</ul>
<h1 id="Auto-Differentiation-with-torch-autograd"><a href="#Auto-Differentiation-with-torch-autograd" class="headerlink" title="Auto Differentiation with torch.autograd"></a>Auto Differentiation with <code>torch.autograd</code></h1><h2 id="属性-1"><a href="#属性-1" class="headerlink" title="属性"></a>属性</h2><ul>
<li>是否可以求导: <code>requires_grad</code> <div class="note note-warning">
            <p>自定义叶子节点默认为 <code>False</code>, 非叶子节点默认为 <code>True</code>, 神经网络参数默认为 <code>True</code>. 判断哪些节点是 <code>True/False</code> 的一个原则: 从需要求导的叶子节点到 loss 节点之间是一条可求导的通路</p>
          </div>
两种指定方式: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor(<span class="hljs-number">1.</span>).requires_grad_()<br>x = torch.tensor(<span class="hljs-number">1.</span>, requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li>
<li>运算名称: <code>grad_fn</code></li>
<li>是否为叶子节点: <code>is_leaf</code></li>
<li>导数值: <code>grad</code></li>
</ul>
<h2 id="梯度求解两种方式"><a href="#梯度求解两种方式" class="headerlink" title="梯度求解两种方式"></a>梯度求解两种方式</h2><h3 id="backward"><a href="#backward" class="headerlink" title="backward()"></a>backward()</h3><p>只计算满足下面条件 Tensor 的梯度 (grad)</p>
<ul>
<li>叶子节点</li>
<li>requires_grad &#x3D; True</li>
<li>依赖该 Tensor 的所有 Tensor 的 <code>requires_grad=True</code></li>
</ul>
<p>backward() 默认为累加梯度, 当多次求导或进行求解高阶导数时, 需要手动将前一次的梯度清零</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pyhton">x.grad.zero_()<br># 在神经网络中<br>optimizer.zero_grad()<br></code></pre></td></tr></table></figure>

<h3 id="autograd-grad"><a href="#autograd-grad" class="headerlink" title="autograd.grad()"></a>autograd.grad()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">grad = torch.autograd.grad(outputs=y, inputs=x)[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 取第一个元素</span><br></code></pre></td></tr></table></figure>

<h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><ul>
<li><code>retain_graph</code>: 当需要计算多个变量的梯度时, 需要设置为 True (调用 autograd.grad&#x2F;backward 后梯度图默认会释放)</li>
<li><code>create_graph</code>: 当需要利用得到的梯度计算高阶梯度是, 需要设置为 True, 即保留当前梯度的计算方式</li>
</ul>
<h2 id="向量求导"><a href="#向量求导" class="headerlink" title="向量求导"></a>向量求导</h2><p><strong>通常情况下, 只能标量对标量, 标量对向量求梯度</strong>. 若是需要向量求梯度, 需要先转化为标量 (而<strong>求和</strong>对分别求导没有影响). 可用代码: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">y.<span class="hljs-built_in">sum</span>().backward()<br>y.backword(torch.ones_like(y))<br><br>grad_x = torch.autograd.grad(outputs=y.<span class="hljs-built_in">sum</span>(), inputs=x)[<span class="hljs-number">0</span>]<br>grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure>

<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>By default, all tensors with <code>requires_grad=True</code> are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have thained the model and just want to apply it to some input data, i.e. we only want to <em>forward</em> computations through the network. We can stop tracking computations by surrounding our computiation code with <code>torch.no_grad()</code> block<br>Another way to achieve the same result is to use the <code>detach()</code> method on the tensor<br>There are reasons you might want to disable gradient tracking:</p>
<ul>
<li>To mark some parameters in your neural network as <strong>frozen parameters</strong>.</li>
<li>To <strong>speed up computations</strong> when you are only doing forward pass, because computations on tensors that do not track gardients would be more efficient</li>
</ul>
<p>In a forward pass, autograd does two things simultaneously:</p>
<ul>
<li>run the requested operation to compute a resulting tensor</li>
<li>maintain the operation’s <em>gradient function</em> in the DAG (dicected acyclic graph)</li>
</ul>
<p>The backward pass kicks off when <code>.backward()</code> is called on the DAG root, <code>autograd</code> then:</p>
<ul>
<li>computes the gradients from each <code>.grad_fn</code>,</li>
<li>accumulates them in the respective tensor’s <code>.grad</code> attribute</li>
<li>using the chain rule, propagates all the way to the leaf tensors</li>
</ul>
<p>For a vector functon, a gradient of function to parameters is given by <strong>Jacobian matrix</strong>. Instead of computing the Jacobian matrix itself, PyTorch allows you to compute <strong>Jacobian Product</strong> $v^T\cdot J$ for a given input column tensor $v$. This achieved by calling <code>backward</code> with $v$ as an argument. The size of $v$ should be the same as the size of the vector function.</p>
<h2 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/279758736#:~:text=PyTorc">撒旦-cc. 一文解释 PyTorch求导相关 (backward, autograd.grad). 知乎</a></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Coding/" class="category-chain-item">Coding</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Python/" class="print-no-link">#Python</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>PyTorch Notes</div>
      <div>https://zonglin-tian.github.io/2024/04/22/PyTorch-Notes/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Temm</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/29/Hexo-Fluid-%E5%8D%9A%E5%AE%A2%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/" title="Hexo Fluid 博客常用配置">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Hexo Fluid 博客常用配置</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/19/Ubuntu-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/" title="Ubuntu 文件操作">
                        <span class="hidden-mobile">Ubuntu 文件操作</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'photon-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zonglin-tian/blog_comments');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <div> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <a href="/hexo-fluid/" target="_self" rel="nofollow noopener"><i class="iconfont icon-love"></i></a> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <a href="/magic/" target="_self" rel="nofollow noopener"><i class="iconfont icon-cards"></i></a>&nbsp;<span>Magic</span> </div> <div> <i class="iconfont icon-ai250"></i> <a href="/inspiring/" target="_self" rel="nofollow noopener">W</a><a href="/writing/" target="_self" rel="nofollow noopener"><span>riting</span></a> <a href="/reading/" target="_self" rel="nofollow noopener"><span>Reading</span></a> <a href="/running/" target="_self" rel="nofollow noopener"><span>Running</span></a> <a href="/meditation/" target="_self" rel="nofollow noopener"><span>Meditati</span></a><a href="/discipline/" target="_self" rel="nofollow noopener"><span>o</span></a><span>n</span> <i class="iconfont icon-shiwutubiao-07"></i> </div> 
    </div>
  
  
    <div style="display:block" class="statistics">
  <span id="timeDate">运行天数...</span>
  <script src="/js/web_run_time.js"></script>

  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        访问 
        <span id="leancloud-site-pv" style="display: none"></span>
        <span id="leancloud-site-pv-trans">转化的 pv 数...</span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        访客 
        <span id="leancloud-site-uv" style="display: none"></span>
        <span id="leancloud-site-uv-trans">转化的 uv 数...</span>
        <a href="/soul-mate/" target="_self" rel="nofollow noopener"> 人</a>&ensp;<i class="iconfont icon-lianaikongjian"></i>
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  var relativeDate = function() {
    var updatedTime = document.getElementById('updated-time');
    if (updatedTime) {
      var text = updatedTime.textContent;
      var reg = /\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})/;
      var matchs = text.match(reg);
      if (matchs) {
        var relativeTime = moment(matchs[0]).fromNow();
        updatedTime.textContent = text.replace(reg, relativeTime);
      }
      updatedTime.style.display = '';
    }
  };
  Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/moment.min.js', function() {
    if (!'zh-cn'.startsWith('en')) {
      Fluid.utils.createScript('https://lib.baomitu.com/moment.js/2.29.4/locale/zh-cn.min.js', function() {
        relativeDate();
      });
    } else {
      relativeDate();
    }
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  <script src="/js/writing_statement.js"></script><script src="/js/deploy_time.js"></script>

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
